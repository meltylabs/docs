---
title: 'Local Models'
description: 'Run Ollama and LMStudio models on Chorus'
icon: 'lock'
---

<img src="/images/local-light.png" alt="Local models" className='block dark:hidden' />
<img src="/images/local-dark.png" alt="Local models" className="hidden dark:block" />

Chorus supports local models running via Ollama or LMStudio.

## Ollama

First, set up [Ollama](https://ollama.com). Then run it:

```bash Run Ollama
ollama serve
```

Next, open the model picker (<kbd>âŒ˜</kbd> + <kbd>J</kbd>) and scroll to the `LOCAL` section. You may need to click the <Icon icon="arrows-rotate"/> refresh icon.

## LMStudio

[todo]